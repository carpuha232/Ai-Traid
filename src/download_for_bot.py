#!/usr/bin/env python3
"""
–§–∏–Ω–∞–ª—å–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö Binance –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–≥–æ –±–æ—Ç–∞
–°–∫–∞—á–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ —Å 1 –∏—é–ª—è 2024 –ø–æ –∏—é–Ω—å 2025 –≥–æ–¥–∞
"""

import os
import requests
import zipfile
import pandas as pd
from datetime import datetime, timedelta
import time
import logging
from pathlib import Path

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('download_bot_log.txt'),
        logging.StreamHandler()
    ]
)

# –¢–æ–ø-30 –º–æ–Ω–µ—Ç –ø–æ –∫–∞–ø–∏—Ç–∞–ª–∏–∑–∞—Ü–∏–∏ (USDT –ø–∞—Ä—ã)
TOP_30_COINS = [
    'BTCUSDT', 'ETHUSDT', 'BNBUSDT', 'ADAUSDT', 'SOLUSDT',
    'XRPUSDT', 'DOTUSDT', 'DOGEUSDT', 'AVAXUSDT', 'MATICUSDT',
    'LINKUSDT', 'UNIUSDT', 'LTCUSDT', 'BCHUSDT', 'XLMUSDT',
    'ATOMUSDT', 'ETCUSDT', 'FILUSDT', 'TRXUSDT', 'NEARUSDT',
    'ALGOUSDT', 'VETUSDT', 'ICPUSDT', 'FTMUSDT', 'MANAUSDT',
    'SANDUSDT', 'AXSUSDT', 'GALAUSDT', 'ROSEUSDT', 'CHZUSDT'
]

# –¢–∞–π–º—Ñ—Ä–µ–π–º—ã –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
TIMEFRAMES = ['5m', '15m', '30m', '1h', '2h', '4h']

# –ë–∞–∑–æ–≤—ã–π URL –¥–ª—è –¥–∞–Ω–Ω—ã—Ö Binance
BASE_URL = "https://data.binance.vision/data/spot/monthly/klines"

def create_directories():
    """–°–æ–∑–¥–∞–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –±–æ—Ç–∞"""
    # –û—Å–Ω–æ–≤–Ω–∞—è –ø–∞–ø–∫–∞ –¥–ª—è –¥–∞–Ω–Ω—ã—Ö –±–æ—Ç–∞
    base_dir = Path("trading_bot/data/historical")
    base_dir.mkdir(parents=True, exist_ok=True)
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
    for timeframe in TIMEFRAMES:
        (base_dir / timeframe).mkdir(exist_ok=True)
    
    return base_dir

def check_file_exists(url):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞ –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ"""
    try:
        response = requests.head(url, timeout=10)
        return response.status_code == 200
    except:
        return False

def download_file(url, filepath):
    """–°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
    try:
        response = requests.get(url, stream=True, timeout=30)
        response.raise_for_status()
        
        with open(filepath, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
        
        return True
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {url}: {e}")
        return False

def extract_zip(zip_path, extract_to):
    """–†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ—Ç ZIP —Ñ–∞–π–ª"""
    try:
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_to)
        return True
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏ {zip_path}: {e}")
        return False

def process_csv_to_parquet(csv_path, parquet_path):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç CSV –≤ Parquet –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —á—Ç–µ–Ω–∏—è"""
    try:
        # –ß–∏—Ç–∞–µ–º CSV —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –∫–æ–ª–æ–Ω–∫–∞–º–∏
        columns = [
            'open_time', 'open', 'high', 'low', 'close', 'volume',
            'close_time', 'quote_volume', 'trades', 'taker_buy_base',
            'taker_buy_quote', 'ignore'
        ]
        
        df = pd.read_csv(csv_path, names=columns, header=None)
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏ –∏—Å–ø—Ä–∞–≤–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
        # –ï—Å–ª–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏ —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–µ, –¥–µ–ª–∏–º –Ω–∞ 1000
        if df['open_time'].max() > 1e12:  # –ï—Å–ª–∏ –±–æ–ª—å—à–µ 12 —Ü–∏—Ñ—Ä
            df['open_time'] = df['open_time'] // 1000
            df['close_time'] = df['close_time'] // 1000
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
        df['open_time'] = pd.to_datetime(df['open_time'], unit='ms', errors='coerce')
        df['close_time'] = pd.to_datetime(df['close_time'], unit='ms', errors='coerce')
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ –¥–∞—Ç–∞–º–∏
        df = df.dropna(subset=['open_time', 'close_time'])
        
        if len(df) == 0:
            logging.warning(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {csv_path}")
            return False
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        numeric_columns = ['open', 'high', 'low', 'close', 'volume', 
                          'quote_volume', 'taker_buy_base', 'taker_buy_quote']
        for col in numeric_columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
        
        df['trades'] = pd.to_numeric(df['trades'], errors='coerce')
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –≤ –≤–∞–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö
        df = df.dropna(subset=['open', 'high', 'low', 'close', 'volume'])
        
        if len(df) == 0:
            logging.warning(f"–ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ –¥–ª—è {csv_path}")
            return False
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ Parquet
        df.to_parquet(parquet_path, index=False)
        
        # –£–¥–∞–ª—è–µ–º CSV —Ñ–∞–π–ª –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞
        os.remove(csv_path)
        
        logging.info(f"‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫")
        return True
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {csv_path}: {e}")
        return False

def get_available_months():
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–µ—Å—è—Ü—ã –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è"""
    available_months = []
    
    # 2024 –≥–æ–¥ - —Å –∏—é–ª—è –ø–æ –¥–µ–∫–∞–±—Ä—å
    for month in range(7, 13):  # –ò—é–ª—å-–¥–µ–∫–∞–±—Ä—å 2024
        available_months.append((2024, month))
    
    # 2025 –≥–æ–¥ - —Å —è–Ω–≤–∞—Ä—è –ø–æ –∏—é–Ω—å
    for month in range(1, 7):  # –Ø–Ω–≤–∞—Ä—å-–∏—é–Ω—å 2025
        test_url = f"{BASE_URL}/BTCUSDT/1h/BTCUSDT-1h-2025-{month:02d}.zip"
        if check_file_exists(test_url):
            available_months.append((2025, month))
        else:
            logging.info(f"–§–∞–π–ª—ã –∑–∞ {month}/2025 –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è")
            break
    
    return available_months

def download_bot_data():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±–æ—Ç–∞"""
    base_dir = create_directories()
    
    logging.info(f"–ù–∞—á–∏–Ω–∞–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–≥–æ –±–æ—Ç–∞")
    logging.info(f"–ü–µ—Ä–∏–æ–¥: 1 –∏—é–ª—è 2024 - –∏—é–Ω—å 2025")
    logging.info(f"–ú–æ–Ω–µ—Ç—ã: {len(TOP_30_COINS)}")
    logging.info(f"–¢–∞–π–º—Ñ—Ä–µ–π–º—ã: {TIMEFRAMES}")
    
    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–µ—Å—è—Ü–µ–≤
    available_months = get_available_months()
    logging.info(f"–î–æ—Å—Ç—É–ø–Ω—ã—Ö –º–µ—Å—è—Ü–µ–≤: {len(available_months)}")
    logging.info(f"–ú–µ—Å—è—Ü—ã: {available_months}")
    
    total_files = len(TOP_30_COINS) * len(TIMEFRAMES) * len(available_months)
    downloaded_files = 0
    
    for coin in TOP_30_COINS:
        logging.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {coin}...")
        
        for timeframe in TIMEFRAMES:
            timeframe_dir = base_dir / timeframe
            coin_dir = timeframe_dir / coin
            coin_dir.mkdir(exist_ok=True)
            
            for year, month in available_months:
                month_str = f"{month:02d}"
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º URL –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è
                filename = f"{coin}-{timeframe}-{year}-{month_str}.zip"
                url = f"{BASE_URL}/{coin}/{timeframe}/{filename}"
                
                zip_path = coin_dir / filename
                csv_filename = f"{coin}-{timeframe}-{year}-{month_str}.csv"
                csv_path = coin_dir / csv_filename
                parquet_filename = f"{coin}-{timeframe}-{year}-{month_str}.parquet"
                parquet_path = coin_dir / parquet_filename
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –µ—Å–ª–∏ —Ñ–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                if parquet_path.exists():
                    logging.info(f"–§–∞–π–ª {parquet_filename} —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    downloaded_files += 1
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å —Ñ–∞–π–ª–∞
                if not check_file_exists(url):
                    logging.warning(f"–§–∞–π–ª {filename} –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º")
                    continue
                
                # –°–∫–∞—á–∏–≤–∞–µ–º ZIP —Ñ–∞–π–ª
                logging.info(f"–°–∫–∞—á–∏–≤–∞–µ–º {filename}...")
                if download_file(url, zip_path):
                    # –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º
                    if extract_zip(zip_path, coin_dir):
                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ Parquet
                        if process_csv_to_parquet(csv_path, parquet_path):
                            # –£–¥–∞–ª—è–µ–º ZIP —Ñ–∞–π–ª
                            os.remove(zip_path)
                            downloaded_files += 1
                            logging.info(f"‚úì –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω {filename}")
                        else:
                            logging.error(f"‚úó –û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ {filename}")
                    else:
                        logging.error(f"‚úó –û—à–∏–±–∫–∞ —Ä–∞—Å–ø–∞–∫–æ–≤–∫–∏ {filename}")
                else:
                    logging.error(f"‚úó –û—à–∏–±–∫–∞ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è {filename}")
                
                # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                time.sleep(0.5)
        
        logging.info(f"–ó–∞–≤–µ—Ä—à–µ–Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∞ {coin}")
    
    logging.info(f"–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {downloaded_files}/{total_files}")

def create_data_summary():
    """–°–æ–∑–¥–∞–µ—Ç —Å–≤–æ–¥–∫—É –ø–æ —Å–∫–∞—á–∞–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º"""
    base_dir = Path("trading_bot/data/historical")
    summary = []
    
    for timeframe in TIMEFRAMES:
        timeframe_dir = base_dir / timeframe
        if not timeframe_dir.exists():
            continue
            
        for coin_dir in timeframe_dir.iterdir():
            if coin_dir.is_dir():
                parquet_files = list(coin_dir.glob("*.parquet"))
                if parquet_files:
                    try:
                        # –ß–∏—Ç–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞ –æ–±—â–µ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—Ç—Ä–æ–∫
                        total_rows = 0
                        min_date = None
                        max_date = None
                        
                        for f in parquet_files:
                            df = pd.read_parquet(f)
                            total_rows += len(df)
                            if min_date is None or df['open_time'].min() < min_date:
                                min_date = df['open_time'].min()
                            if max_date is None or df['open_time'].max() > max_date:
                                max_date = df['open_time'].max()
                        
                        summary.append({
                            'coin': coin_dir.name,
                            'timeframe': timeframe,
                            'files_count': len(parquet_files),
                            'total_rows': total_rows,
                            'date_range': f"{min_date.date() if min_date else 'N/A'} - {max_date.date() if max_date else 'N/A'}"
                        })
                    except Exception as e:
                        logging.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {coin_dir}: {e}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–≤–æ–¥–∫—É
    if summary:
        summary_df = pd.DataFrame(summary)
        summary_df.to_csv("trading_bot/data/data_summary.csv", index=False)
        logging.info("–°–≤–æ–¥–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ trading_bot/data/data_summary.csv")
        
        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        print("\n=== –°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–ö–ê–ß–ê–ù–ù–´–• –î–ê–ù–ù–´–• ===")
        print(f"–ü–µ—Ä–∏–æ–¥: 1 –∏—é–ª—è 2024 - –∏—é–Ω—å 2025")
        print(f"–í—Å–µ–≥–æ –º–æ–Ω–µ—Ç: {summary_df['coin'].nunique()}")
        print(f"–í—Å–µ–≥–æ —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤: {summary_df['timeframe'].nunique()}")
        print(f"–í—Å–µ–≥–æ —Ñ–∞–π–ª–æ–≤: {summary_df['files_count'].sum()}")
        print(f"–í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫ –¥–∞–Ω–Ω—ã—Ö: {summary_df['total_rows'].sum():,}")
        print(f"–î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: trading_bot/data/historical/")

def create_data_loader():
    """–°–æ–∑–¥–∞–µ—Ç –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±–æ—Ç–∞"""
    loader_code = '''
import pandas as pd
from pathlib import Path
from typing import List, Optional
import logging

class HistoricalDataLoader:
    """–ó–∞–≥—Ä—É–∑—á–∏–∫ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–≥–æ –±–æ—Ç–∞"""
    
    def __init__(self, data_path: str = "trading_bot/data/historical"):
        self.data_path = Path(data_path)
        self.logger = logging.getLogger(__name__)
    
    def get_coin_data(self, coin: str, timeframe: str, start_date: Optional[str] = None, 
                     end_date: Optional[str] = None) -> pd.DataFrame:
        """
        –ó–∞–≥—Ä—É–∂–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–Ω–µ—Ç—ã –∏ —Ç–∞–π–º—Ñ—Ä–µ–π–º–∞
        
        Args:
            coin: –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–Ω–µ—Ç—ã (–Ω–∞–ø—Ä–∏–º–µ—Ä, 'BTCUSDT')
            timeframe: –¢–∞–π–º—Ñ—Ä–µ–π–º ('5m', '15m', '30m', '1h', '2h', '4h')
            start_date: –ù–∞—á–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ 'YYYY-MM-DD'
            end_date: –ö–æ–Ω–µ—á–Ω–∞—è –¥–∞—Ç–∞ –≤ —Ñ–æ—Ä–º–∞—Ç–µ 'YYYY-MM-DD'
        
        Returns:
            DataFrame —Å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
        """
        coin_dir = self.data_path / timeframe / coin
        
        if not coin_dir.exists():
            self.logger.error(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {coin_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            return pd.DataFrame()
        
        # –ß–∏—Ç–∞–µ–º –≤—Å–µ parquet —Ñ–∞–π–ª—ã
        parquet_files = list(coin_dir.glob("*.parquet"))
        if not parquet_files:
            self.logger.error(f"–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {coin} {timeframe}")
            return pd.DataFrame()
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã
        dfs = []
        for file in sorted(parquet_files):
            df = pd.read_parquet(file)
            dfs.append(df)
        
        if not dfs:
            return pd.DataFrame()
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
        combined_df = pd.concat(dfs, ignore_index=True)
        combined_df = combined_df.sort_values('open_time').reset_index(drop=True)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –¥–∞—Ç–∞–º –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω—ã
        if start_date is not None:
            start_dt = pd.to_datetime(start_date)
            combined_df = combined_df[combined_df['open_time'] >= start_dt]
        if end_date is not None:
            end_dt = pd.to_datetime(end_date)
            combined_df = combined_df[combined_df['open_time'] <= end_dt]
        
        self.logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(combined_df)} —Å—Ç—Ä–æ–∫ –¥–ª—è {coin} {timeframe}")
        return combined_df
    
    def get_available_coins(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–Ω–µ—Ç"""
        coins = []
        for timeframe_dir in self.data_path.iterdir():
            if timeframe_dir.is_dir():
                for coin_dir in timeframe_dir.iterdir():
                    if coin_dir.is_dir() and coin_dir.name not in coins:
                        coins.append(coin_dir.name)
        return sorted(coins)
    
    def get_available_timeframes(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤"""
        timeframes = []
        for timeframe_dir in self.data_path.iterdir():
            if timeframe_dir.is_dir():
                timeframes.append(timeframe_dir.name)
        return sorted(timeframes)

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
if __name__ == "__main__":
    loader = HistoricalDataLoader()
    
    # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ BTCUSDT –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –º–µ—Å—è—Ü
    btc_data = loader.get_coin_data('BTCUSDT', '1h', start_date='2025-05-01')
    print(f"BTC –¥–∞–Ω–Ω—ã–µ: {len(btc_data)} —Å—Ç—Ä–æ–∫")
    
    # –°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–Ω–µ—Ç
    coins = loader.get_available_coins()
    print(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–Ω–µ—Ç—ã: {len(coins)}")
    
    # –°–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–∞–π–º—Ñ—Ä–µ–π–º–æ–≤
    timeframes = loader.get_available_timeframes()
    print(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–∞–π–º—Ñ—Ä–µ–π–º—ã: {timeframes}")
'''
    
    # –°–æ–∑–¥–∞–µ–º —Ñ–∞–π–ª –∑–∞–≥—Ä—É–∑—á–∏–∫–∞
    loader_path = Path("trading_bot/data_loader.py")
    loader_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(loader_path, 'w', encoding='utf-8') as f:
        f.write(loader_code)
    
    logging.info(f"–°–æ–∑–¥–∞–Ω –∑–∞–≥—Ä—É–∑—á–∏–∫ –¥–∞–Ω–Ω—ã—Ö: {loader_path}")

if __name__ == "__main__":
    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–æ—Ä–≥–æ–≤–æ–≥–æ –±–æ—Ç–∞")
    print("üìÖ –ü–µ—Ä–∏–æ–¥: 1 –∏—é–ª—è 2024 - –∏—é–Ω—å 2025")
    print("üìÅ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω—è—é—Ç—Å—è –≤ trading_bot/data/historical/")
    print("=" * 60)
    
    download_bot_data()
    create_data_summary()
    create_data_loader()
    
    print("\n‚úÖ –°–∫–∞—á–∏–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    print("üìÅ –î–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ 'trading_bot/data/historical/'")
    print("üìä –°–≤–æ–¥–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ 'trading_bot/data/data_summary.csv'")
    print("üîß –ó–∞–≥—Ä—É–∑—á–∏–∫ —Å–æ–∑–¥–∞–Ω: 'trading_bot/data_loader.py'")
    print("üìù –õ–æ–≥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ 'download_bot_log.txt'")
    print("\nüí° –î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –≤ –±–æ—Ç–µ:")
    print("   from trading_bot.data_loader import HistoricalDataLoader")
    print("   loader = HistoricalDataLoader()")
    print("   data = loader.get_coin_data('BTCUSDT', '1h')") 